{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULijP03Am70E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import explained_variance_score,mean_squared_error,mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, LSTM, Dropout, BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.experimental import Adamax, Adadelta, Adagrad, Ftrl, Nadam, RMSprop, SGD\n",
        "from keras.models import load_model\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2yQaT-ueqss",
        "outputId": "796497cb-85ad-4ded-c562-98314071b487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrW5B7iCnk8r"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/weather_2017+2018_hourly.csv\")\n",
        "data_as_np = dataset.GHI.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa1FwhbMnk_X"
      },
      "outputs": [],
      "source": [
        "def series_to_supervised(data_np,n_lags,n_outs):\n",
        "    samples = len(data_np)-(n_lags+n_outs-1)\n",
        "    Inputs = []\n",
        "    Targets = np.empty((samples,n_outs))\n",
        "    for i in range(samples):\n",
        "        step = i+n_lags\n",
        "        sample = data_np[i:step]\n",
        "        Inputs.append(sample)\n",
        "        label = data_np[step:(step+n_outs)]\n",
        "        j = 0\n",
        "        for a in label:\n",
        "            Targets[i,j] = a\n",
        "            j = j+1\n",
        "    return np.array(Inputs),Targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LFT-ybwnlCO"
      },
      "outputs": [],
      "source": [
        "def plot_model_history(model_summary):\n",
        "    plt.plot(model_summary.history['loss'])\n",
        "    plt.plot(model_summary.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train','test'],loc='upper right')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIQoGFmpnlHW"
      },
      "outputs": [],
      "source": [
        "data_np = data_as_np.reshape(-1,1)\n",
        "def prepare_model(n_lags):\n",
        "  n_out = 24\n",
        "\n",
        "  # Prepare data\n",
        "  Inputs,Targets = series_to_supervised(data_np,n_lags,n_out)\n",
        "\n",
        "  Inputs_model, Targets_model = Inputs[:-10], Targets[:-10]\n",
        "  Inputs_plot, Targets_plot = Inputs[-10:], Targets[-10:]\n",
        "\n",
        "  no_samples = Targets_model.shape[0]\n",
        "  split = int(0.15 * no_samples)\n",
        "\n",
        "  # Splitting data into train and test data set\n",
        "  Inputs_model = Inputs_model.reshape((no_samples,n_lags,1))\n",
        "\n",
        "  train_x,train_y = Inputs_model[:-split],Targets_model[:-split]\n",
        "  test_x,test_y = Inputs_model[-split:],Targets_model[-split:]\n",
        "  return train_x,train_y,test_x,test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZWPT658nlJw"
      },
      "outputs": [],
      "source": [
        "def rmse_error(pred,y):\n",
        "    return np.sqrt(mean_squared_error(pred,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4aBsMa0dzAe"
      },
      "outputs": [],
      "source": [
        "def create_model(input_shape,n_out,train_x,train_y,test_x,test_y):\n",
        "    # define model\n",
        "    inputs = Input(shape = input_shape)\n",
        "    # input layer...\n",
        "    lstm1 = LSTM(64, activation = 'relu')(inputs)\n",
        "    #dr1 = Dropout(0.1)(lstm1)\n",
        "    #lstm2 = LSTM(64, activation = 'relu')(lstm1)\n",
        "    #dr2 = Dropout(0.1)(lstm1)(lstm2)\n",
        "    #output layer...\n",
        "    output = Dense(n_out,activation = 'relu')(lstm1)\n",
        "    model = Model(inputs = inputs, outputs = output)\n",
        "\n",
        "    #cp1 = ModelCheckpoint('StackLSTM/', save_best_only=True)\n",
        "    model.compile(loss='mean_squared_error', optimizer = Adam(0.0001), metrics=[RootMeanSquaredError()])\n",
        "    model_summary = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=70, batch_size=64) #callbacks=[cp1]\n",
        "    plot_model_history(model_summary)\n",
        "\n",
        "    #model = load_model('StackLSTM/')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s11DRyAod21D"
      },
      "outputs": [],
      "source": [
        "train_rmse_error, test_rmse_error, test_mae = 0,0,0\n",
        "data_np = data_as_np.reshape(-1,1)\n",
        "n_lags = 60\n",
        "n_out = 24\n",
        "# Prepare data\n",
        "Inputs,Targets = series_to_supervised(data_np,n_lags,n_out)\n",
        "\n",
        "Inputs_model, Targets_model = Inputs[:-10], Targets[:-10]\n",
        "Inputs_plot, Targets_plot = Inputs[-10:], Targets[-10:]\n",
        "\n",
        "no_samples = Targets_model.shape[0]\n",
        "split = int(0.3 * no_samples)\n",
        "\n",
        "input_shape = (n_lags,1)\n",
        "\n",
        "train_x,train_y = Inputs_model[:-split],Targets_model[:-split]\n",
        "test_x,test_y = Inputs_model[-split:],Targets_model[-split:]\n",
        "for _ in range(4):\n",
        "  model = create_model(input_shape,n_out,train_x,train_y,test_x,test_y)\n",
        "\n",
        "  train_pred,test_pred = model.predict(train_x), model.predict(test_x)\n",
        "\n",
        "  train_rmse_error = rmse_error(train_pred.flatten(),train_y.flatten())\n",
        "  test_rmse_error = rmse_error(test_pred.flatten(),test_y.flatten())\n",
        "  test_mae = mean_absolute_error(test_pred.flatten(),test_y.flatten())\n",
        "  with open(\"/content/drive/MyDrive/LSTMerror1.txt\",\"a\") as f:\n",
        "    f.write(f\"{train_rmse_error},{test_rmse_error},{test_mae}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}