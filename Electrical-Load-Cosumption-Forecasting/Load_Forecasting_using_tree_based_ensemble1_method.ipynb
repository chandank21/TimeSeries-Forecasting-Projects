{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score,mean_squared_error,mean_absolute_percentage_error\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    raw_df=[]\n",
    "    df=pd.read_excel(r'C:\\Users\\Chandan\\Desktop\\Analog_Reports\\report_ss4_CheLab_2.xlsx',\n",
    "                skiprows=6,parse_dates={'Time':[0]},index_col='Time',skipfooter=1,sheet_name=None)\n",
    "    for key,dframe in df.items():\n",
    "        dframe.drop(dframe.tail(1).index,inplace=True)\n",
    "        raw_df.append(dframe[['KW']])\n",
    "    raw_df=pd.concat(raw_df)\n",
    "    raw_df.index=pd.to_datetime(raw_df.index,format='%d-%m-%Y %H:%M')\n",
    "    return raw_df\n",
    "\n",
    "def load_weather():\n",
    "    weather = pd.read_csv(r'C:\\Users\\Chandan\\Desktop\\Analog_Reports\\kanpur_weather.csv',\n",
    "                             header=24)\n",
    "    LIST=[]\n",
    "    for i in range(weather.shape[0]):\n",
    "        if weather.HR.values[i] < 10:\n",
    "            string = f\"{weather.YEAR.values[i]}-{weather.MO.values[i]}-{weather.DY.values[i]} {0}{weather.HR.values[i]}\"\n",
    "        else:\n",
    "            string = f\"{weather.YEAR.values[i]}-{weather.MO.values[i]}-{weather.DY.values[i]} {weather.HR.values[i]}\"\n",
    "\n",
    "        LIST.append(pd.to_datetime(string,format='%Y-%m-%d %H'))\n",
    "\n",
    "    weather['time']=LIST\n",
    "    weather=weather.set_index(weather['time'],drop=False)\n",
    "    weather.drop(['YEAR','MO','DY','HR','time'],axis=1,inplace=True)\n",
    "    return weather\n",
    "\n",
    "def fill_missing(df,neighbours=6):\n",
    "    start,end=df.index[0],df.index[-1]\n",
    "    index=pd.date_range(start=start,end=end,freq='30T')\n",
    "    t_df=pd.DataFrame({'val':np.NaN},index=index)\n",
    "    result = df.join(t_df, how=\"outer\").drop(['val'],axis=1)\n",
    "    imputer = KNNImputer(n_neighbors=neighbours)\n",
    "    imputed = imputer.fit_transform(result)\n",
    "    return pd.DataFrame({'KW':imputed.flatten()},index=index)\n",
    "\n",
    "def adding_hour_feature(data,hour_lags=5):\n",
    "    data_index= data.index\n",
    "    for i in range(1,(hour_lags+1)):\n",
    "        for index in data_index:\n",
    "            if index-timedelta(hours=i) in data.index:\n",
    "                data.loc[index,f\"t_{i}\"]=data.loc[index-timedelta(hours=i),'KW']\n",
    "            else:\n",
    "                data.loc[index,f\"t_{i}\"]=data.loc[index-timedelta(hours=i)+timedelta(days=1),'KW']\n",
    "    return data\n",
    "\n",
    "def adding_day_feature(data,day_lags=1):\n",
    "    data_index= data.index\n",
    "    for i in range(1,(day_lags+1)):\n",
    "        for index in data_index:\n",
    "            if index-timedelta(days=i) in data.index:\n",
    "                a = data.loc[index-timedelta(hours=24*i),'KW']\n",
    "                b = data.loc[index-timedelta(hours=24*i-1),'KW']\n",
    "                if index-timedelta(hours=24*i+1) in data.index:\n",
    "                    c = data.loc[index-timedelta(hours=24*i+1),'KW'] \n",
    "                else:\n",
    "                    c= data.loc[index-timedelta(hours=24*i),'KW']\n",
    "                data.loc[index,f\"d_{i}\"] = np.mean((a,b,c)) \n",
    "            else:\n",
    "                data.loc[index,f\"d_{i}\"] = data.loc[index-timedelta(days=i)+timedelta(weeks=1),'KW']\n",
    "    return data\n",
    "\n",
    "def add_timing_feature(dff,hour_lags=5,day_lags=1):\n",
    "                              \n",
    "    dff = adding_hour_feature(dff,hour_lags=hour_lags)\n",
    "    dff = adding_day_feature(dff,day_lags=day_lags)  \n",
    "                              \n",
    "    dff['day_of_week']=dff.index.day_name()\n",
    "    dff['hour']=dff.index.hour\n",
    "    \n",
    "    # adding cyclicity feature...\n",
    "    #dff['hour_cos'] = np.cos(2 * np.pi * dff['hour'] / 24)\n",
    "    #dff['hour_sin'] = np.sin(2 * np.pi * dff['hour'] / 24)\n",
    "    \n",
    "    # adding hour wise mean...\n",
    "    for hour,hour_df in dff.groupby('hour'):\n",
    "        dff.loc[dff[dff.hour==hour].index,'hour_mean']=hour_df[-7:].KW.mean()\n",
    "    \n",
    "    #for daywise mean....\n",
    "    for day,day_df in dff.groupby('day_of_week'):\n",
    "        dff.loc[dff[dff.day_of_week==day].index,'week_mean'] = day_df[-5:].KW.mean()\n",
    "        \n",
    "    return dff.drop(columns=['day_of_week','hour'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df=load_data()\n",
    "imputed_df = fill_missing(raw_df)\n",
    "hourly_df=imputed_df.resample('1H').mean()\n",
    "hourly_dff=add_timing_feature(hourly_df,hour_lags=6,day_lags=5)\n",
    "weather_df=load_weather()\n",
    "combine_df = pd.merge(hourly_dff, weather_df, left_index=True, right_index=True)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_df = scaler.fit_transform(combine_df.drop(columns=['KW'],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying PCA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3092, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.99)\n",
    "X= scaled_df\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53827018, 0.14030178, 0.10644566, 0.06715712, 0.05913008,\n",
       "       0.04423583, 0.01072645, 0.00841584, 0.0070201 , 0.00452925,\n",
       "       0.00321742, 0.00289248])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.39052080e+00, -4.58164647e-01,  6.56466102e-02, ...,\n",
       "         2.75326924e-02, -2.02386744e-03,  2.86352504e-03],\n",
       "       [ 1.39109152e+00, -4.86096934e-01,  6.30309930e-02, ...,\n",
       "         3.08008676e-02,  1.62272791e-02, -1.70639332e-02],\n",
       "       [ 1.39263705e+00, -5.17626413e-01,  6.06139989e-02, ...,\n",
       "        -1.40818291e-03, -7.29730575e-03, -3.24299739e-02],\n",
       "       ...,\n",
       "       [-6.86528020e-01,  8.76367628e-01,  2.93244724e+00, ...,\n",
       "        -9.33164476e-03,  2.43462548e-02, -5.18647074e-02],\n",
       "       [-6.84128883e-01,  7.04777220e-01,  2.94844080e+00, ...,\n",
       "        -7.26536606e-02,  7.55789426e-03, -3.52704652e-03],\n",
       "       [-6.80657958e-01,  5.39490718e-01,  2.96069879e+00, ...,\n",
       "        -1.07288852e-01, -1.13105291e-02,  1.27689089e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = X_pca\n",
    "target_data = combine_df.KW.values\n",
    "inp_dim = int(0.7 * X_pca.shape[0])\n",
    "train_input = input_data[:inp_dim]\n",
    "train_target = target_data[:inp_dim]\n",
    "test_input = input_data[inp_dim:]\n",
    "test_target = target_data[inp_dim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 909 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "base_model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "base_model.fit(train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set model score....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9829568003241742"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = base_model.predict(test_input)\n",
    "train_result = base_model.predict(train_input)\n",
    "print(f\"Training set model score....\")\n",
    "base_model.score(train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set model score....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8421762931920035"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Test set model score....\")\n",
    "base_model.score(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...summary\n",
      "explained_variance_score =0.9811925116869648\n",
      "mean_squared_error =1.7751955379930473\n",
      "mean_absolute_percentage_error =0.01785985501244579\n"
     ]
    }
   ],
   "source": [
    "def print_summary():\n",
    "    print(f\"test...summary\")\n",
    "    print(f\"explained_variance_score ={explained_variance_score(train_result,train_target)}\")\n",
    "    print(f\"mean_squared_error ={mean_squared_error(train_result,train_target)}\")\n",
    "    print(f\"mean_absolute_percentage_error ={mean_absolute_percentage_error(train_result,train_target)}\")\n",
    "print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...summary\n",
      "explained_variance_score =0.8313332868808613\n",
      "mean_squared_error =13.173732231497295\n",
      "mean_absolute_percentage_error =0.062120855565392756\n"
     ]
    }
   ],
   "source": [
    "def print_summary():\n",
    "    print(f\"test...summary\")\n",
    "    print(f\"explained_variance_score ={explained_variance_score(test_result,test_target)}\")\n",
    "    print(f\"mean_squared_error ={mean_squared_error(test_result,test_target)}\")\n",
    "    print(f\"mean_absolute_percentage_error ={mean_absolute_percentage_error(test_result,test_target)}\")\n",
    "print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importance_df = pd.DataFrame({\n",
    "    'feature': combine_df.columns[1:],\n",
    "    'importance': base_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('estimator=90',\n",
       "  'test_acc=0.8426267012290392',\n",
       "  'mean_absolute_percentage_error =0.062191532109316126'),\n",
       " ('estimator=100',\n",
       "  'test_acc=0.8421762931920035',\n",
       "  'mean_absolute_percentage_error =0.062120855565392756'),\n",
       " ('estimator=110',\n",
       "  'test_acc=0.842256389861469',\n",
       "  'mean_absolute_percentage_error =0.06220996482109334'),\n",
       " ('estimator=120',\n",
       "  'test_acc=0.8423940502867032',\n",
       "  'mean_absolute_percentage_error =0.062129082820372175')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(max_estimator):\n",
    "    acc_list=[]\n",
    "    for estimator in max_estimator:\n",
    "        model = RandomForestRegressor(random_state=42, n_jobs=-1, n_estimators=estimator)\n",
    "        model.fit(train_input, train_target)\n",
    "        test_result = model.predict(test_input)\n",
    "        acc_list.append((f\"estimator={estimator}\",\n",
    "                        f\"test_acc={model.score(test_input, test_target)}\",\n",
    "                        f\"mean_absolute_percentage_error ={mean_absolute_percentage_error(test_result,test_target)}\"))\n",
    "    return acc_list\n",
    "get_score([90,100,110,120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maxdepth=30',\n",
       "  'test_acc=0.8421762931920035',\n",
       "  'mean_absolute_percentage_error =0.06212085556539275'),\n",
       " ('maxdepth=35',\n",
       "  'test_acc=0.8421762931920035',\n",
       "  'mean_absolute_percentage_error =0.062120855565392756'),\n",
       " ('maxdepth=40',\n",
       "  'test_acc=0.8421762931920035',\n",
       "  'mean_absolute_percentage_error =0.06212085556539275')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(max_depth):\n",
    "    acc_list=[]\n",
    "    for depth in max_depth:\n",
    "        model=RandomForestRegressor(random_state=42, n_jobs=-1,n_estimators=100,max_depth=depth)\n",
    "        model.fit(train_input, train_target)\n",
    "        test_result = model.predict(test_input)\n",
    "        acc_list.append((f\"maxdepth={depth}\",\n",
    "                         f\"test_acc={model.score(test_input, test_target)}\",\n",
    "                         f\"mean_absolute_percentage_error ={mean_absolute_percentage_error(test_result,test_target)}\"))\n",
    "    return acc_list\n",
    "get_score([30,35,40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('max_leaf_nodes=2**10',\n",
       "  'test_acc=0.8440674361602364',\n",
       "  'mean_absolute_percentage_error =0.06196448574495167'),\n",
       " ('max_leaf_nodes=2**15',\n",
       "  'test_acc=0.8440892155382209',\n",
       "  'mean_absolute_percentage_error =0.061959658379459624'),\n",
       " ('max_leaf_nodes=2**20',\n",
       "  'test_acc=0.8440892155382209',\n",
       "  'mean_absolute_percentage_error =0.061959658379459624')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(max_leaf_node):\n",
    "    acc_list=[]\n",
    "    for leaf in max_leaf_node:\n",
    "        model=RandomForestRegressor(random_state=42,\n",
    "                                    n_jobs=-1,\n",
    "                                    n_estimators=100,\n",
    "                                    max_depth=30,\n",
    "                                    max_leaf_nodes=2**leaf)\n",
    "        model.fit(train_input, train_target)\n",
    "        test_result = model.predict(test_input)\n",
    "        acc_list.append((f\"max_leaf_nodes=2**{leaf}\",\n",
    "                        f\"test_acc={model.score(test_input, test_target)}\",\n",
    "                        f\"mean_absolute_percentage_error ={mean_absolute_percentage_error(test_result,test_target)}\"))\n",
    "    return acc_list\n",
    "get_score([10,15,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9510701877032838, 0.8417313582704616)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RandomForestRegressor(random_state=42,\n",
    "                                    n_jobs=-1,\n",
    "                                    n_estimators=100,\n",
    "                                    max_depth=30,\n",
    "                                    max_leaf_nodes=2**10,\n",
    "                                   max_samples=0.5,\n",
    "                               min_impurity_decrease=1e-6)\n",
    "model.fit(train_input, train_target)\n",
    "model.score(train_input, train_target),model.score(test_input, test_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
